# Anti-Patterns Checklist - UPDATED
## LLM Coding Tutor Platform (CodeMentor)

**Document Version:** 1.1 (Updated)
**Created:** 2025-12-06
**Updated:** 2025-12-06
**Purpose:** Prevent recurring architectural and security anti-patterns during development
**Status:** Updated to reflect SEC-2 and SEC-2-AUTH completion

---

## How to Use This Checklist

This checklist documents anti-patterns found in the codebase and provides guidelines to avoid them in future development. Review this checklist:

- **Before starting new features:** Check relevant sections for patterns to avoid
- **During code review:** Verify new code doesn't introduce listed anti-patterns
- **Monthly:** Review and update with newly discovered patterns

**Severity Levels:**
- **CRITICAL (P0):** Deployment blocker, must fix immediately
- **HIGH (P1):** Security/performance risk, fix before production
- **MEDIUM (P2):** Maintainability issue, fix in next sprint
- **LOW (P3):** Nice-to-have improvement, backlog

**Status Indicators:**
- ‚úÖ **RESOLVED** - Anti-pattern fixed, serves as example for future
- ‚ö†Ô∏è **FOUND** - Anti-pattern present, needs remediation
- üìù **GUIDANCE** - General pattern to avoid

---

## 1. Security Anti-Patterns

### ‚úÖ RESOLVED: AP-SEC-002 Placeholder Security Implementation

**What:** Decorator or security check that logs but doesn't enforce
**Why It's Bad:** False sense of security, allows unauthorized access
**Status:** ‚úÖ **RESOLVED** - SEC-2-AUTH work stream (2025-12-06)

**Example (BAD - BEFORE):**
```python
def require_verified_email(function: Callable) -> Callable:
    @wraps(function)
    async def wrapper(*args, **kwargs):
        # TODO: In a full implementation, we would fetch user from database
        logger.debug("Email verification check (placeholder)")
        return await function(*args, **kwargs)  # ‚ö†Ô∏è Doesn't actually check!
    return wrapper
```

**Solution Implemented (GOOD - AFTER):**
```python
def require_verified_email(function: Callable) -> Callable:
    @wraps(function)
    async def wrapper(*args, **kwargs):
        from src.models.user import User
        from src.utils.database import get_async_db_session as get_session
        from sqlalchemy import select

        if not hasattr(g, "user_id"):
            raise APIError("Authentication required", status_code=401)

        # ACTUALLY CHECK EMAIL VERIFICATION
        async with get_session() as session:
            result = await session.execute(
                select(User).where(User.id == g.user_id)
            )
            user = result.scalar_one_or_none()

            if not user or not user.email_verified:
                logger.warning(
                    "Access denied: email not verified",
                    extra={"user_id": g.user_id, "path": request.path}
                )
                raise APIError(
                    "Email verification required. Please verify your email address.",
                    status_code=403
                )

        return await function(*args, **kwargs)
    return wrapper
```

**Key Learnings:**
- Never merge security code with "TODO" or "placeholder" comments
- Write tests BEFORE implementing security features (TDD)
- Security features must be complete or not exist (no partial implementations)
- Always perform actual validation (database lookup, API call, etc.)

**Evidence:** `/backend/src/middleware/auth_middleware.py:222-289`

---

### ‚úÖ RESOLVED: AP-SEC-003 Configuration Validation Missing

**What:** Starting production server without validating environment variables
**Why It's Bad:** Runtime failures, security issues (weak secrets)
**Status:** ‚úÖ **RESOLVED** - SEC-2 work stream (2025-12-06)

**Example (BAD - BEFORE):**
```python
class Settings(BaseSettings):
    secret_key: SecretStr = Field(..., env="SECRET_KEY")
    database_url: str = Field(..., env="DATABASE_URL")
    # No validation - accepts any value!
```

**Solution Implemented (GOOD - AFTER):**
```python
class Settings(BaseSettings):
    secret_key: SecretStr = Field(..., env="SECRET_KEY")
    database_url: str = Field(..., env="DATABASE_URL")

    @field_validator("secret_key", "jwt_secret_key")
    @classmethod
    def validate_secret_strength(cls, value: SecretStr) -> SecretStr:
        secret_str = value.get_secret_value()
        if len(secret_str) < 32:
            raise ValueError(
                f"Secret key must be at least 32 characters long for security. "
                f"Got {len(secret_str)} characters."
            )
        return value

    @model_validator(mode='after')
    def validate_production_config(self) -> 'Settings':
        if self.app_env != "production":
            return self

        # Detect development secrets
        dev_secret_patterns = ["changeme", "password", "secret", "test", "development"]
        for secret_field in ["secret_key", "jwt_secret_key"]:
            secret_value = getattr(self, secret_field).get_secret_value().lower()
            for pattern in dev_secret_patterns:
                if pattern in secret_value:
                    raise ValueError(
                        f"Production {secret_field} appears to be a development secret!"
                    )

        # Require HTTPS for URLs in production
        if not self.frontend_url.startswith("https://"):
            raise ValueError("FRONTEND_URL must use HTTPS in production")

        # Validate database URL format
        if not self.database_url.startswith(("postgresql://", "postgres://")):
            raise ValueError("DATABASE_URL must be a valid PostgreSQL connection string")

        # Require LLM API key if primary provider is GROQ
        if self.llm_primary_provider == "groq" and not self.groq_api_key:
            raise ValueError("GROQ_API_KEY is required in production")

        return self
```

**Key Learnings:**
- Validate all critical settings at startup (fail fast)
- Check secrets aren't development defaults in production
- Require HTTPS in production
- Validate URL formats for databases, Redis, etc.
- Provide clear error messages to guide configuration fixes

**Evidence:** `/backend/src/config.py:120-232`

---

### ‚ö†Ô∏è PARTIALLY RESOLVED: AP-SEC-001 Secrets in Git Repository

**What:** Committing secrets, API keys, or production URLs to git
**Why It's Bad:** Public exposure, difficult to rotate, compliance violations
**Status:** ‚ö†Ô∏è **PARTIALLY RESOLVED** - Pre-commit hooks added, history cleanup pending

**Current State:**
- ‚úÖ Pre-commit hooks prevent future `.env` commits (SEC-2)
- ‚úÖ `.env.example` created with safe placeholders
- ‚úÖ Configuration validation rejects weak secrets
- ‚ö†Ô∏è Git history cleanup not yet performed (BFG Repo-Cleaner)
- ‚ö†Ô∏è Secrets manager (GCP/AWS) not yet configured

**Example (BAD):**
```bash
# .env (tracked in git!)
SECRET_KEY=mysecretkey123
DATABASE_URL=postgresql://user:password@localhost/db
GROQ_API_KEY=gsk_abc123...
```

**Current Solution (GOOD):**
```bash
# ‚úÖ .gitignore includes:
.env
*.env
!.env.example

# ‚úÖ .env.example with placeholders:
SECRET_KEY=your-secret-key-here-min-32-chars
DATABASE_URL=postgresql://user:password@localhost:5432/dbname
GROQ_API_KEY=your-groq-api-key-here

# ‚úÖ Pre-commit hook prevents commits:
# .pre-commit-config.yaml
repos:
  - repo: https://github.com/Yelp/detect-secrets
    hooks:
      - id: detect-secrets
        args: ['--baseline', '.secrets.baseline']
```

**Remaining Work:**
```bash
# 1. Purge .env from git history
git filter-repo --path .env --invert-paths

# 2. Rotate all exposed secrets
python -c 'import secrets; print(secrets.token_urlsafe(32))'  # New SECRET_KEY
# Change DATABASE_URL password
# Rotate GROQ_API_KEY

# 3. Set up secrets manager
# GCP Secret Manager:
gcloud secrets create SECRET_KEY --data-file=/dev/stdin
gcloud secrets create DATABASE_URL --data-file=/dev/stdin
```

**Key Learnings:**
- NEVER commit `.env` files to git
- Use `.env.example` with placeholder values
- Implement pre-commit hooks to prevent future mistakes
- Purge secrets from git history before going public
- Use cloud secrets manager (GCP Secret Manager, AWS Secrets Manager) for production

**Evidence:** `.gitignore` updated, `.env.example` present

---

### ‚ö†Ô∏è FOUND: AP-SEC-004 Rate Limiting Missing on Expensive Endpoints

**What:** No rate limits on endpoints that call expensive external APIs
**Why It's Bad:** Cost abuse, DoS attacks, budget exhaustion
**Status:** ‚ö†Ô∏è **FOUND** - SEC-3 work stream in progress

**Example (BAD - CURRENT):**
```python
@chat_bp.route("/send", methods=["POST"])
@require_auth
async def send_message():
    # ‚ö†Ô∏è No rate limit! User can spam LLM calls
    message = data["message"]
    response = await LLMService.generate_response(message)  # $0.01-0.10 per call
    return jsonify({"response": response}), 200
```

**Solution (GOOD - TO IMPLEMENT):**
```python
from src.middleware.rate_limiter import llm_rate_limit

@chat_bp.route("/send", methods=["POST"])
@require_auth
@llm_rate_limit("chat")  # ‚úÖ Tiered rate limiting by user role
async def send_message():
    message = data["message"]
    response = await LLMService.generate_response(message)
    return jsonify({"response": response}), 200
```

**Rate Limiting Implementation Available:**
- `llm_rate_limit` decorator exists in `/backend/src/middleware/rate_limiter.py`
- Supports tiered limits: Free (10/min), Admin (30/min)
- Includes cost tracking and daily cost limits
- Just needs to be applied to endpoints

**Endpoints Requiring Rate Limits:**
- `/api/chat/send` - LLM calls ($$$)
- `/api/exercises/generate` - LLM exercise generation ($$$)
- `/api/exercises/{id}/hint` - LLM hints ($$$)
- `/api/auth/login` - Prevent brute force
- `/api/auth/register` - Prevent spam

**Recommendation:** Apply `llm_rate_limit` decorator in SEC-3 work stream (3 days)

---

### ‚ö†Ô∏è FOUND: AP-SEC-005 Input Validation Inconsistency

**What:** Some endpoints validate input with Pydantic, others use manual checks
**Why It's Bad:** Injection attacks, XSS, data corruption
**Status:** ‚ö†Ô∏è **FOUND** - SEC-3-INPUT work stream planned

**Example (INCONSISTENT):**
```python
# ‚úÖ Good: Uses Pydantic schema
@exercises_bp.route("/", methods=["POST"])
@require_auth
async def create_exercise():
    data = CreateExerciseRequest(**await request.get_json())  # Validated!
    exercise = await ExerciseService.create(data)
    return jsonify(exercise), 201

# ‚ùå Bad: Manual validation
@chat_bp.route("/send", methods=["POST"])
@require_auth
async def send_message():
    data = await request.get_json()
    message = data.get("message")  # No validation!
    if not message:
        return jsonify({"error": "Message required"}), 400
```

**Solution (GOOD):**
```python
from pydantic import BaseModel, Field, field_validator
import bleach

class SendMessageRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=10000)
    conversation_id: Optional[int] = None

    @field_validator('message')
    @classmethod
    def sanitize_message(cls, v: str) -> str:
        # Sanitize to prevent XSS
        allowed_tags = ['b', 'i', 'code', 'pre', 'a']
        cleaned = bleach.clean(v, tags=allowed_tags, strip=True)
        return cleaned.strip()

@chat_bp.route("/send", methods=["POST"])
@require_auth
async def send_message():
    data = SendMessageRequest(**await request.get_json())  # ‚úÖ Validated!
    response = await LLMService.generate_response(data.message)
    return jsonify(response), 200
```

**Best Practices:**
- ALWAYS use Pydantic for request validation
- Define `min_length` and `max_length` on all string fields
- Use `regex` for enums/patterns (email, phone, etc.)
- Custom validators for complex business logic
- Sanitize user-generated content (bleach library)

**Recommendation:** Audit all 32 endpoints in SEC-3-INPUT work stream (5 days)

---

## 2. Architecture Anti-Patterns

### ‚úÖ RESOLVED: AP-ARCH-004 Dual Database Engines

**What:** Creating both sync and async database engines
**Why It's Bad:** Double connection pool usage, wasted resources
**Status:** ‚úÖ **RESOLVED** - DB-OPT work stream (2025-12-06)

**Example (BAD - BEFORE):**
```python
class DatabaseManager:
    def __init__(self):
        # ‚ùå Both engines created (40 connections total!)
        self._sync_engine = create_engine(database_url)  # 20 connections
        self._async_engine = create_async_engine(async_url)  # 20 connections
```

**Solution Implemented (GOOD - AFTER):**
```python
class DatabaseManager:
    def __init__(self):
        # ‚úÖ Only async engine (20 connections)
        self._async_engine = create_async_engine(
            async_url,
            pool_size=pool_size,
            max_overflow=max_overflow
        )

def get_sync_engine_for_migrations(database_url: str):
    """Create sync engine ONLY for Alembic migrations."""
    return create_engine(
        database_url,
        poolclass=QueuePool,
        pool_size=5,  # Small pool for migrations only
        max_overflow=5
    )
```

**Performance Improvement:**
- Connection pool: **40 ‚Üí 20 connections (50% reduction)**
- Admin queries: **800ms ‚Üí 12ms (67x faster)**

**Key Learnings:**
- In async applications, use async engines only
- Create sync engines only when absolutely necessary (migrations)
- Use small pool sizes for infrequent operations
- Document connection pool sizing formula: `workers √ó threads √ó 2 + overhead`

**Evidence:** `/backend/src/utils/database.py:30-98`, `/backend/src/utils/database.py:249-275`

---

### üìù GUIDANCE: AP-ARCH-001 God Object (Large Service Classes)

**What:** Service class with 500+ lines doing too many things
**Why It's Bad:** Hard to test, maintain, understand
**Status:** ‚ö†Ô∏è **FOUND** (Low Priority) - ExerciseService (600+ lines), ProgressService (720+ lines)

**Example (CURRENT - ACCEPTABLE FOR MVP):**
```python
class ExerciseService:
    """Handles exercise generation, evaluation, hints, difficulty."""

    def generate_exercise(self):  # 50 lines
    def evaluate_solution(self):  # 40 lines
    def generate_hint(self):  # 30 lines
    def adjust_difficulty(self):  # 40 lines
    # ... 15 methods, 600+ lines total
```

**Future Refactoring (GOOD - LOW PRIORITY):**
```python
# ‚úÖ Split into focused services (SRP)
class ExerciseGenerator:
    """Responsible only for generating new exercises."""
    def generate_personalized_exercise(self, user_profile): pass
    def generate_from_template(self, template_id): pass

class ExerciseEvaluator:
    """Responsible only for evaluating solutions."""
    def evaluate_solution(self, exercise_id, solution): pass
    def calculate_score(self, test_results): pass

class HintService:
    """Responsible only for generating hints."""
    def generate_hint(self, exercise_id, user_attempt): pass
```

**Rule of Thumb:**
- Service class should be <300 lines (guideline, not strict rule)
- Class should have one primary responsibility
- If class name needs "and" or "Manager", consider splitting

**Recommendation:** Low priority - current implementation acceptable for MVP. Refactor if classes exceed 1000 lines or when adding major features.

---

### ‚ö†Ô∏è FOUND: AP-ARCH-002 Magic Numbers

**What:** Hardcoded values scattered throughout code
**Why It's Bad:** Hard to change, inconsistencies, unclear intent
**Status:** ‚ö†Ô∏è **FOUND** - Partially addressed in config, but some remain

**Example (BAD):**
```python
redis_client.setex(key, 3600, value)  # Why 3600?
if len(password) < 12:  # Why 12?
@rate_limit(60, 10)  # What do these mean?
```

**Solution (GOOD):**
```python
# config.py or constants.py
class CacheTTL:
    USER_PROFILE = 300  # 5 minutes
    EXERCISE_TEMPLATE = 3600  # 1 hour
    LLM_RESPONSE = 7200  # 2 hours

class PasswordPolicy:
    MIN_LENGTH = 12
    REQUIRE_UPPERCASE = True
    REQUIRE_LOWERCASE = True

class RateLimits:
    FREE_TIER_PER_MINUTE = 60
    FREE_TIER_BURST = 10

# Usage
redis_client.setex(key, CacheTTL.USER_PROFILE, value)
if len(password) < PasswordPolicy.MIN_LENGTH:
    raise APIError("Password too short")
```

**Common Magic Numbers to Extract:**
- Rate limits (requests per minute/hour)
- Cache TTLs (time to live)
- Pagination limits (page size, max results)
- Timeouts (connection, read, write)
- Password/validation rules

**Recommendation:** Extract to constants class or config (Medium priority)

---

### ‚ö†Ô∏è FOUND: AP-ARCH-003 Missing Pagination

**What:** Endpoints return all results without pagination
**Why It's Bad:** Memory exhaustion, slow responses
**Status:** ‚ö†Ô∏è **FOUND** - PERF-1 work stream planned

**Example (BAD):**
```python
@exercises_bp.route("/history", methods=["GET"])
@require_auth
async def get_exercise_history():
    # ‚ùå Returns ALL exercises (could be thousands!)
    result = await session.execute(
        select(UserExercise).where(UserExercise.user_id == g.user_id)
    )
    exercises = result.scalars().all()
    return jsonify([e.to_dict() for e in exercises]), 200
```

**Solution (GOOD):**
```python
class PaginationParams(BaseModel):
    limit: int = Field(50, ge=1, le=100)
    offset: int = Field(0, ge=0)

@exercises_bp.route("/history", methods=["GET"])
@require_auth
async def get_exercise_history():
    params = PaginationParams(**request.args)

    # Get paginated results
    result = await session.execute(
        select(UserExercise)
        .where(UserExercise.user_id == g.user_id)
        .order_by(UserExercise.created_at.desc())
        .limit(params.limit)
        .offset(params.offset)
    )
    exercises = result.scalars().all()

    # Get total count
    total = await session.scalar(
        select(func.count()).select_from(UserExercise)
        .where(UserExercise.user_id == g.user_id)
    )

    return jsonify({
        "data": [e.to_dict() for e in exercises],
        "pagination": {
            "total": total,
            "limit": params.limit,
            "offset": params.offset,
            "has_next": (params.offset + params.limit) < total
        }
    }), 200
```

**Pagination Checklist:**
- [ ] Default limit (e.g., 50 items)
- [ ] Maximum limit enforced (e.g., 100 items)
- [ ] Return total count for UI
- [ ] Return `has_next` for infinite scroll
- [ ] Sort by stable field (id, created_at)

**Affected Endpoints:**
- `/api/exercises/history`
- `/api/chat/conversations`
- `/api/progress/history`

**Recommendation:** Implement in PERF-1 work stream (2 days)

---

## 3. Performance Anti-Patterns

### ‚úÖ RESOLVED: AP-PERF-002 Missing Database Indexes

**What:** Queries on columns without indexes
**Why It's Bad:** Full table scans, slow queries
**Status:** ‚úÖ **RESOLVED** - DB-OPT work stream (2025-12-06)

**Indexes Added:**
```sql
CREATE INDEX idx_users_role ON users(role);
CREATE INDEX idx_users_email_verified ON users(email_verified);
CREATE INDEX idx_users_is_active ON users(is_active);
CREATE INDEX idx_exercises_difficulty ON exercises(difficulty);
CREATE INDEX idx_exercises_language ON exercises(language);
CREATE INDEX idx_user_exercises_user_created ON user_exercises(user_id, created_at);
CREATE INDEX idx_user_exercises_status ON user_exercises(status);
```

**Performance Improvements:**
- Admin queries: **800ms ‚Üí 12ms (67x faster)**
- Exercise generation: **400ms ‚Üí 6ms (67x faster)**
- Streak calculations: **1200ms ‚Üí 25ms (48x faster)**

**Checklist for New Tables:**
- [ ] Index on foreign keys (user_id, exercise_id)
- [ ] Index on frequently queried fields (status, role, is_active)
- [ ] Composite index on WHERE + ORDER BY combinations
- [ ] Index on date fields for filtering/sorting

**Key Learnings:**
- Index columns used in WHERE clauses
- Create composite indexes for multi-column queries
- Don't over-index (each index slows writes)
- Rule of thumb: 3-5 indexes per table

**Evidence:** `/backend/alembic/versions/20251206_add_missing_indexes_db_opt.py`

---

### ‚ö†Ô∏è FOUND: AP-PERF-003 No Query Result Caching

**What:** Every request hits database even for static data
**Why It's Bad:** Unnecessary database load
**Status:** ‚ö†Ô∏è **FOUND** - PERF-1 work stream planned

**Example (BAD):**
```python
@users_bp.route("/profile", methods=["GET"])
@require_auth
async def get_profile():
    # ‚ùå Hits database every time
    user = await session.get(User, g.user_id)
    return jsonify(user.to_dict()), 200
```

**Solution (GOOD):**
```python
@users_bp.route("/profile", methods=["GET"])
@require_auth
async def get_profile():
    redis = get_redis()
    cache_key = f"user_profile:{g.user_id}"

    # ‚úÖ Try cache first
    cached = await redis.get_cache(cache_key)
    if cached:
        return jsonify(cached), 200

    # Cache miss -> query database
    user = await session.get(User, g.user_id)
    profile_data = user.to_dict()

    # Store in cache (TTL: 5 minutes)
    await redis.set_cache(cache_key, profile_data, ttl=300)

    return jsonify(profile_data), 200
```

**Data to Cache:**
- User profiles (TTL: 5 minutes)
- Exercise templates (TTL: 1 hour)
- Achievement definitions (TTL: 24 hours)
- LLM responses (TTL: 2 hours) - **Already implemented**

**Cache Invalidation:**
- Update profile -> invalidate `user_profile:{user_id}`
- Create exercise -> invalidate user exercise history cache

**Recommendation:** Implement in PERF-1 work stream (2 days)

---

## 4. Testing Anti-Patterns

### üìù GUIDANCE: AP-TEST-001 Mocking Too Much

**What:** Mocking entire database or all external dependencies
**Why It's Bad:** Tests pass but integration bugs happen in production
**Status:** ‚úÖ **GUIDANCE FOLLOWED** - Integration tests prioritize real DB

**Example (BAD):**
```python
def test_create_exercise():
    # ‚ùå Mocks everything
    with patch('src.utils.database.get_async_db_session'):
        with patch('src.services.exercise_service.ExerciseService.create'):
            # Test passes but doesn't catch DB issues!
```

**Solution (GOOD - CURRENT APPROACH):**
```python
@pytest.mark.asyncio
async def test_create_exercise(test_db):
    # ‚úÖ Use real test database
    async with get_async_db_session() as session:
        exercise = await ExerciseService.create(
            user_id=1,
            title="Test Exercise",
            description="Test"
        )

        # Verify in database (real integration test)
        result = await session.get(Exercise, exercise.id)
        assert result.title == "Test Exercise"
```

**When to Mock:**
- ‚úÖ External APIs (LLM, GitHub, email)
- ‚úÖ Time/date functions
- ‚úÖ Random number generators
- ‚ùå Database (use test database)
- ‚ùå Redis (use test Redis or fakeredis)
- ‚ùå Internal services

**Current Status:** ‚úÖ Tests follow this guidance (per CLAUDE.md)

---

### ‚ö†Ô∏è FOUND: AP-TEST-002 No E2E Tests

**What:** Only unit tests, no full user journey tests
**Why It's Bad:** Integration bugs not caught
**Status:** ‚ö†Ô∏è **FOUND** - QA-1 Phase 4 planned

**Recommendation:**
```typescript
// ‚úÖ Playwright E2E test
import { test, expect } from '@playwright/test';

test('user registration to first exercise', async ({ page }) => {
    // Navigate to registration
    await page.goto('http://localhost:3000/register');

    // Fill form
    await page.fill('input[name="email"]', 'test@example.com');
    await page.fill('input[name="password"]', 'SecureP@ss123');
    await page.click('button[type="submit"]');

    // Verify email verification page
    await expect(page).toHaveURL(/.*verify-email.*/);

    // Complete onboarding
    await page.goto('/verify?token=test-token');
    await page.fill('select[name="language"]', 'Python');
    await page.click('button[type="submit"]');

    // Verify first exercise shown
    await expect(page.locator('h1')).toContainText('Daily Exercise');
});
```

**Critical E2E Flows:**
1. Registration ‚Üí Verification ‚Üí Onboarding ‚Üí First Exercise
2. Login ‚Üí Chat ‚Üí Hint ‚Üí Submission
3. OAuth flow
4. Password reset

**Recommendation:** Implement in QA-1 Phase 4 (10 days)

---

## 5. Updated Code Review Checklist

Use this during code reviews:

### Security Review:
- [ ] No secrets committed to git
- [ ] All security decorators fully implemented (no placeholders) ‚úÖ
- [ ] Input validation with Pydantic schemas
- [ ] Rate limiting on expensive endpoints
- [ ] Authentication required on protected routes ‚úÖ
- [ ] Authorization checks for resource access ‚úÖ
- [ ] Email verification enforced where needed ‚úÖ
- [ ] SQL injection protection ‚úÖ
- [ ] XSS protection (output encoding, sanitization)
- [ ] CSRF protection (SameSite cookies) ‚úÖ

### Architecture Review:
- [ ] No God objects (service classes <300 lines guideline)
- [ ] No magic numbers (constants extracted)
- [ ] Pagination on list endpoints
- [ ] Error codes for error responses
- [ ] Async/await used correctly ‚úÖ
- [ ] No blocking I/O in request handlers ‚úÖ
- [ ] Proper separation of concerns ‚úÖ

### Performance Review:
- [ ] Database indexes on foreign keys and query fields ‚úÖ
- [ ] No N+1 queries (eager loading)
- [ ] Query result caching for static data
- [ ] Pagination implemented
- [ ] Connection pooling configured ‚úÖ

### Testing Review:
- [ ] Tests written (TDD preferred)
- [ ] Integration tests for new features ‚úÖ
- [ ] E2E test for critical user journeys
- [ ] Mock only external APIs ‚úÖ
- [ ] Test coverage tracked

---

## 6. Prevention Strategy - Updated

**Development Phase:**
1. ‚úÖ Pre-commit hooks prevent secret commits
2. ‚úÖ Configuration validation at startup
3. Review this checklist before new features
4. Run automated checks

**Code Review Phase:**
1. Reviewer uses section 5 checklist
2. Automated tools run (linters, security scanners)
3. At least one approval required
4. CI/CD tests must pass

**Deployment Phase:**
1. ‚úÖ Config validation in staging
2. Security scan before production
3. Load testing for critical features
4. Rollback plan documented

---

**End of Updated Checklist**
**Last Updated:** 2025-12-06
**Next Review:** After SEC-3 completion or monthly
